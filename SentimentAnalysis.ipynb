{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NIcWyy-wyhF"
   },
   "source": [
    "> **Data Mining Programming Project**:\n",
    ">\n",
    "> - Konstantinos Konstantinidis AEM:2546 konkonstant@uth.gr\n",
    "> - Nikolaos Stavrinos AEM:2631 nstavrinos@uth.gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:18:56.808580Z",
     "start_time": "2021-05-15T13:18:50.428057Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riO22wemwyhe",
    "outputId": "dab0bb90-3a27-4097-b437-82a18db9b24f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\programdata\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.6.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.59.0)\n",
      "Requirement already satisfied: tweepy in c:\\programdata\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (2.25.1)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (1.15.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Requirement already satisfied: wordcloud in c:\\programdata\\anaconda3\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (8.2.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.1)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Requirement already satisfied: langdetect in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.9)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from langdetect) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# Install Libraries\n",
    "!pip install textblob\n",
    "!pip install tweepy\n",
    "!pip install wordcloud\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhQl7n_kwyhi"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:18:58.990595Z",
     "start_time": "2021-05-15T13:18:56.811027Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5wGcvA-wyhk",
    "outputId": "b0ce0527-60b6-4fef-ac27-89d14acabde5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from langdetect import detect\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FG08p6atwyhl"
   },
   "source": [
    "# Download all the lexicons needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:18:59.361484Z",
     "start_time": "2021-05-15T13:18:58.993170Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J3v4l4kewyhm",
    "outputId": "bf8cd638-6294-4168-dafe-42cc0cdcd882"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grqagTDYwyho"
   },
   "source": [
    "# Tweeter Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:18:59.378622Z",
     "start_time": "2021-05-15T13:18:59.369017Z"
    },
    "id": "dkkuskamwyhp"
   },
   "outputs": [],
   "source": [
    "#Â Authentication\n",
    "consumerKey = \"xebmbbEHiMzamPFMBhoEkW8N8\"\n",
    "consumerSecret = \"irADKXxRXLoDlWP90DulNUN9kVQr7pfICTQlPA4BRbhUu9PZTj\"\n",
    "accessToken = \"3053418828-Kqwl2veNMbb9hpbJ2Zdax8tVm6V9zBfRTGtB7Bt\"\n",
    "accessTokenSecret = \"q6ZYSCCPlwhOoxV2QBCSqjRT0GOS941qCQQMbJWoy5wuk\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9lUJOUHswyhq"
   },
   "source": [
    "# Sentiment Analysis without data preprocessing\n",
    "## Ask user for topic and word count input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:27.351186Z",
     "start_time": "2021-05-15T13:18:59.388358Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_k9PWWS9wyhq",
    "outputId": "d1d66628-a0ee-413a-f25d-660b07ccbeac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter keyword or hashtag to search: data mining\n",
      "Please enter how many tweets to analyze: 1500\n"
     ]
    },
    {
     "ename": "TweepError",
     "evalue": "Twitter error response: status code = 429",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTweepError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b734df24f105>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mpositive_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#print(tweet.text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m             \u001b[1;31m# Reached end of current page, get the next page...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpage_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tweepy\\cursor.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRawParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__self__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    251\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[1;31m# Parse the response payload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTweepError\u001b[0m: Twitter error response: status code = 429"
     ]
    }
   ],
   "source": [
    "\n",
    "def percentage(part,whole):\n",
    "    return 100 * float(part)/float(whole) \n",
    "\n",
    "keyword = input(\"Please enter keyword or hashtag to search: \")\n",
    "noOfTweet = int(input (\"Please enter how many tweets to analyze: \"))\n",
    "\n",
    "\n",
    "tweets = tweepy.Cursor(api.search, q=keyword).items(noOfTweet)\n",
    "positive  = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    \n",
    "    #print(tweet.text)\n",
    "    tweet_list.append(tweet.text)\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    polarity += analysis.sentiment.polarity\n",
    "    \n",
    "    if neg > pos:\n",
    "        negative_list.append(tweet.text)\n",
    "        negative += 1\n",
    "\n",
    "    elif pos > neg:\n",
    "        positive_list.append(tweet.text)\n",
    "        positive += 1\n",
    "    \n",
    "    elif pos == neg:\n",
    "        neutral_list.append(tweet.text)\n",
    "        neutral += 1\n",
    "\n",
    "positive = percentage(positive, noOfTweet)\n",
    "negative = percentage(negative, noOfTweet)\n",
    "neutral = percentage(neutral, noOfTweet)\n",
    "polarity = percentage(polarity, noOfTweet)\n",
    "positive = format(positive, '.1f')\n",
    "negative = format(negative, '.1f')\n",
    "neutral = format(neutral, '.1f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQXdOUkYwyht"
   },
   "source": [
    "# Number of Tweets (Total, Positive, Negative, Neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:27.359567Z",
     "start_time": "2021-05-15T13:20:27.352887Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjMUCDhiwyhu",
    "outputId": "15a4286f-1c12-43ec-e27a-2e5077a62f5a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "neutral_list = pd.DataFrame(neutral_list)\n",
    "negative_list = pd.DataFrame(negative_list)\n",
    "positive_list = pd.DataFrame(positive_list)\n",
    "print(\"total number: \",len(tweet_list))\n",
    "print(\"positive number: \",len(positive_list))\n",
    "print(\"negative number: \", len(negative_list))\n",
    "print(\"neutral number: \",len(neutral_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:27.397266Z",
     "start_time": "2021-05-15T13:20:27.367575Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "muz-1QsHwyhv",
    "outputId": "f551de35-de47-4c5c-af69-448ee52932a5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tweet_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0T5785FWwyhx"
   },
   "source": [
    "# PieChart of current data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:27.653958Z",
     "start_time": "2021-05-15T13:20:27.401026Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "03typkvLwyhx",
    "outputId": "634e5e6f-6cf8-4089-c83c-8208bb8f61e9"
   },
   "outputs": [],
   "source": [
    "labels = ['Positive ['+str(positive)+'%]' , 'Neutral ['+str(neutral)+'%]','Negative ['+str(negative)+'%]']\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = ['green', 'blue','red']\n",
    "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
    "plt.style.use('default')\n",
    "plt.legend(labels)\n",
    "plt.title(\"Sentiment Analysis Result for keyword=  \"+keyword+\"\" )\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:27.663346Z",
     "start_time": "2021-05-15T13:20:27.656820Z"
    },
    "id": "-uwWEAqBwyhz"
   },
   "outputs": [],
   "source": [
    "tweet_list.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA-2uXpiwyh0"
   },
   "source": [
    "# Extracting text values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:27.685184Z",
     "start_time": "2021-05-15T13:20:27.665481Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "G9hK0Rk-wyh1",
    "outputId": "8a4450d2-ec01-417c-99ab-a9027909c6d1"
   },
   "outputs": [],
   "source": [
    "tw_list = pd.DataFrame(tweet_list)\n",
    "tw_list[\"text\"] = tw_list[0]\n",
    "tw_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiI80V-gwyh2"
   },
   "source": [
    "# Cleaning Text (RT, URL ,Emoji etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:27.720225Z",
     "start_time": "2021-05-15T13:20:27.687095Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "FOmWu6nLwyh3",
    "outputId": "217d5200-fe0f-4671-fe3c-7546a72f8df4"
   },
   "outputs": [],
   "source": [
    "#Removing RT, Url,Emojis etc\n",
    "remove_rt = lambda x: re.sub(r'RT|\\@\\w+','', x)\n",
    "rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\"\",x)\n",
    "tw_list[\"text\"] = tw_list.text.map(remove_rt).map(rt)\n",
    "tw_list[\"text\"] = tw_list.text.str.lower()\n",
    "\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnNwKNYGwyiT"
   },
   "source": [
    "# Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:27.735767Z",
     "start_time": "2021-05-15T13:20:27.727371Z"
    },
    "id": "zOXVunH9wyiW"
   },
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    # Remove numbers\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    # Remove punctuations\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    return text\n",
    "\n",
    "tw_list['punct'] = tw_list[\"text\"].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enM-SSOHwyiZ"
   },
   "source": [
    "# Appliyng tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:27.804122Z",
     "start_time": "2021-05-15T13:20:27.740232Z"
    },
    "id": "tydYtoXzwyih"
   },
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "   \n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "tw_list['tokenized'] = tw_list['punct'].apply(lambda x: tokenization(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8S_ecxawyim"
   },
   "source": [
    "# Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:27.823176Z",
     "start_time": "2021-05-15T13:20:27.805863Z"
    },
    "id": "13fxb-L8wyim"
   },
   "outputs": [],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y26oKT3cwyio"
   },
   "source": [
    "# Applying Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:27.884990Z",
     "start_time": "2021-05-15T13:20:27.824506Z"
    },
    "id": "capcYtTKwyip"
   },
   "outputs": [],
   "source": [
    "#Appliyng Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Negative, Positive, Neutral and Compound values after processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:30.190480Z",
     "start_time": "2021-05-15T13:20:27.886633Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "evyCDopMwyh4",
    "outputId": "593c3e7f-c02c-47a6-fefe-80923aa5fe6d"
   },
   "outputs": [],
   "source": [
    "tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in tw_list['text'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        tw_list.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        tw_list.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        tw_list.loc[index, 'sentiment'] = \"neutral\"\n",
    "    tw_list.loc[index, 'neg'] = neg\n",
    "    tw_list.loc[index, 'neu'] = neu\n",
    "    tw_list.loc[index, 'pos'] = pos\n",
    "    tw_list.loc[index, 'compound'] = comp\n",
    "\n",
    "tw_list.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iabEkaWhwyh6"
   },
   "source": [
    "# Creating new data frames for all sentiments (positive, negative and neutral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:30.198562Z",
     "start_time": "2021-05-15T13:20:30.192483Z"
    },
    "id": "u5oekL8Jwyh7"
   },
   "outputs": [],
   "source": [
    "tw_list_negative = tw_list[tw_list[\"sentiment\"]==\"negative\"]\n",
    "tw_list_positive = tw_list[tw_list[\"sentiment\"]==\"positive\"]\n",
    "tw_list_neutral = tw_list[tw_list[\"sentiment\"]==\"neutral\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMB_YvKKwyh8"
   },
   "source": [
    "# Function to count values of sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:30.207968Z",
     "start_time": "2021-05-15T13:20:30.201542Z"
    },
    "id": "jLih3Vtlwyh8"
   },
   "outputs": [],
   "source": [
    "def count_values_in_column(data,feature):\n",
    "    total=data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:30.224175Z",
     "start_time": "2021-05-15T13:20:30.210277Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "id": "Tc7Ttp2cwyh9",
    "outputId": "9f7491c3-5cda-4c7e-9553-506e9d1fc465"
   },
   "outputs": [],
   "source": [
    "#Count_values for sentiment\n",
    "count_values_in_column(tw_list,\"sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmUeyFC1wyh-"
   },
   "source": [
    "# Create Pie Chart for processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:30.543460Z",
     "start_time": "2021-05-15T13:20:30.225922Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "luXK4Mf6wyh_",
    "outputId": "fe855092-e7dd-4add-cfa2-ff3c9603d0b0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pichart = count_values_in_column(tw_list,\"sentiment\")\n",
    "labels = [  'Neutral ['+str(pichart[\"Percentage\"][0])+'%]','Positive ['+str(pichart[\"Percentage\"][1])+'%]','Negative ['+str(pichart[\"Percentage\"][2])+'%]']\n",
    "sizes = pichart[\"Percentage\"]\n",
    "colors = [ 'blue','green','red']\n",
    "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
    "plt.style.use('default')\n",
    "plt.legend(labels)\n",
    "plt.title(\"Sentiment Analysis Result for keyword=  \"+keyword+\"\" )\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGdO2virwyiB"
   },
   "source": [
    "# Function to Create cloud of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:30.552020Z",
     "start_time": "2021-05-15T13:20:30.545423Z"
    },
    "id": "dnwJCgiwwyiC"
   },
   "outputs": [],
   "source": [
    "def create_wordcloud(text):\n",
    "    mask = np.array(Image.open(\"cloud.png\"))\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wc = WordCloud(background_color=\"white\",\n",
    "                  mask = mask,\n",
    "                  max_words=1000,\n",
    "                  stopwords=stopwords,\n",
    "                  repeat=True)\n",
    "    wc.generate(str(text))\n",
    "    wc.to_file(\"wc.png\")\n",
    "    path=\"wc.png\"\n",
    "    display(Image.open(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7k-S3bpwyiD"
   },
   "source": [
    "### Creating wordcloud for all tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:20:48.952876Z",
     "start_time": "2021-05-15T13:20:30.555275Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "id": "gAFOBCluwyiD",
    "outputId": "d4708cea-253e-4b14-fc4f-be3d605e167a"
   },
   "outputs": [],
   "source": [
    "create_wordcloud(tw_list[\"text\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIn80no7wyiE"
   },
   "source": [
    "### Creating wordcloud for positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:21:11.471781Z",
     "start_time": "2021-05-15T13:20:48.957558Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "id": "S8Ji0yQIwyiF",
    "outputId": "0ad13f11-e79d-45a6-eefd-8f96f812d183"
   },
   "outputs": [],
   "source": [
    "create_wordcloud(tw_list_positive[\"text\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWDzve9bwyiG"
   },
   "source": [
    "### Creating wordcloud for negative sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:21:36.779622Z",
     "start_time": "2021-05-15T13:21:11.483938Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "id": "SacX-9YMwyiG",
    "outputId": "fcdba553-80b0-4285-a83c-fde534ddcc63"
   },
   "outputs": [],
   "source": [
    "create_wordcloud(tw_list_negative[\"text\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zctXwElwyiH"
   },
   "source": [
    "### Creating wordcloud for neutral sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:21:57.961451Z",
     "start_time": "2021-05-15T13:21:36.781818Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 962
    },
    "id": "rJaaYGamwyiH",
    "outputId": "0abd01fd-c63e-4ac5-bd8f-5012d28a3e6e"
   },
   "outputs": [],
   "source": [
    "create_wordcloud(tw_list_neutral[\"text\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUmsHaRUwyiH"
   },
   "source": [
    "## Calculating tweet's lenght and word count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:21:57.977141Z",
     "start_time": "2021-05-15T13:21:57.963595Z"
    },
    "id": "UkVvKt3pwyiI"
   },
   "outputs": [],
   "source": [
    "tw_list['text_len'] = tw_list['text'].astype(str).apply(len)\n",
    "tw_list['text_word_count'] = tw_list['text'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:21:57.999371Z",
     "start_time": "2021-05-15T13:21:57.982262Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "7jLWxvicwyiJ",
    "outputId": "52c483ea-8533-41f4-eb83-9c5197123c43",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_len.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:21:58.022298Z",
     "start_time": "2021-05-15T13:21:58.001444Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "FaPVKLxiwyiO",
    "outputId": "96ba1291-3681-49ca-d0c2-17ec073abb20"
   },
   "outputs": [],
   "source": [
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_word_count.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:21:58.053705Z",
     "start_time": "2021-05-15T13:21:58.024421Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "j-jx3dRAwyi5",
    "outputId": "fe5006e2-b98b-44e4-98b9-dbcd0c41701e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tw_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:21:58.232023Z",
     "start_time": "2021-05-15T13:21:58.055980Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJYZj1kqHEFn",
    "outputId": "85d691e1-da86-4ce9-a91c-49ec3117f34e"
   },
   "outputs": [],
   "source": [
    "#Analyzer for CuntVectorizer\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "\n",
    "#converting the sentiment values to 0,1,2 \n",
    "y=tw_list['sentiment']\n",
    "sentiment_ordering = ['positive', 'neutral', 'negative']\n",
    "y = y.apply(lambda x: sentiment_ordering.index(x))\n",
    "\n",
    "#vectorazing the text values\n",
    "cv = CountVectorizer(analyzer=clean_text) \n",
    "X = cv.fit_transform(tw_list['text'])\n",
    "\n",
    "\n",
    "#splittig the data set into training and test set\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3)\n",
    "\n",
    "\n",
    "#Naive Bayes Model\n",
    "NB_model=MultinomialNB()\n",
    "NB_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred_nb=NB_model.predict(X_test)\n",
    "\n",
    "cf=classification_report(y_test,y_pred_nb)\n",
    "print(\"NB\",cf)\n",
    "\n",
    "\n",
    "#Logistic Regression Model\n",
    "LR_model = LogisticRegression(solver='lbfgs')\n",
    "LR_model.fit(X_train, y_train)\n",
    "y_predict_lr = LR_model.predict(X_test)\n",
    "\n",
    "cf=classification_report(y_test, y_predict_lr)\n",
    "print(\"LR\",cf)\n",
    "\n",
    "\n",
    "#SVM Model\n",
    "SVM_model = svm.SVC()\n",
    "SVM_model.fit(X_train, y_train)\n",
    "y_predict_svm = SVM_model.predict(X_test)\n",
    "\n",
    "cf=classification_report(y_test,y_predict_svm)\n",
    "print(\"SVM\",cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miGdE0knwyi9"
   },
   "source": [
    "# Most Used Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hr_fAaXqwyjA"
   },
   "source": [
    "## Horizontal chart for word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:21:58.240172Z",
     "start_time": "2021-05-15T13:21:58.234954Z"
    },
    "id": "Zbj8YCnrwyjA"
   },
   "outputs": [],
   "source": [
    "def ybarchart(n_trigrams):\n",
    "    labels = []\n",
    "    values = []\n",
    "\n",
    "\n",
    "    for a_tuple in n_trigrams:\n",
    "\n",
    "        labels.append(a_tuple[0])\n",
    "        values.append(a_tuple[1])\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.barh(x - width/2, values, width)\n",
    "\n",
    "    ax.set_ylabel('Words')\n",
    "    ax.set_title('Common Words Found in Tweets (Without Stop or Collection Words)')\n",
    "    ax.set_yticks(x)\n",
    "    ax.set_yticklabels(labels)\n",
    "    \n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDiWqe0uwyjB"
   },
   "source": [
    "## Function to find word correlation in same tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:21:58.255174Z",
     "start_time": "2021-05-15T13:21:58.250773Z"
    },
    "id": "W6sxQtmUwyjC"
   },
   "outputs": [],
   "source": [
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS4QxruHwyjD"
   },
   "source": [
    "## 1 word correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:21:58.559010Z",
     "start_time": "2021-05-15T13:21:58.257431Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "id": "b4Aw9bsewyjE",
    "outputId": "1dca8761-c548-48d4-ba64-ead27d4c8cd2"
   },
   "outputs": [],
   "source": [
    "countdf = get_top_n_gram(tw_list['text'],(1,1),10)\n",
    "ybarchart(countdf[1:11])\n",
    "countdf[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGz690g0wyjF"
   },
   "source": [
    "## 2 word correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:21:58.879277Z",
     "start_time": "2021-05-15T13:21:58.560953Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 650
    },
    "id": "68J5SEC9wyjH",
    "outputId": "c71c61cc-ddf7-4397-ba43-05cf54b4835f"
   },
   "outputs": [],
   "source": [
    "#n2_bigram\n",
    "n2_bigrams = get_top_n_gram(tw_list['text'],(2,2),10)\n",
    "ybarchart(n2_bigrams)\n",
    "n2_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIHQ_eY5wyjI"
   },
   "source": [
    "## 3 word correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:21:59.208395Z",
     "start_time": "2021-05-15T13:21:58.882197Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 650
    },
    "id": "F4V2DhnXwyjI",
    "outputId": "14c7ce84-8dba-4c5a-fa60-a1496350f197"
   },
   "outputs": [],
   "source": [
    "n3_trigrams = get_top_n_gram(tw_list['text'],(3,3),10)\n",
    "ybarchart(n3_trigrams)\n",
    "n3_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j598MI8LwyjJ"
   },
   "source": [
    "# Search a specific word in the tweets and with what other words it is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-15T13:26:35.698535Z",
     "start_time": "2021-05-15T13:21:59.211393Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txgeNsyhwyjJ",
    "outputId": "b2b8bccf-4638-4acd-dc42-feeee5a53668"
   },
   "outputs": [],
   "source": [
    "search_word = input(\"Please enter a word to search within the tweets: \")\n",
    "count_search = Counter()\n",
    "\n",
    "for  tweet in tw_list['nonstop']:\n",
    "    terms_only = [term for term in tweet]\n",
    "    \n",
    "    if search_word in terms_only:\n",
    "        count_search.update(terms_only)\n",
    "print(\"Co-occurrence for %s:\" % search_word)\n",
    "print(count_search.most_common(10))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SentimentAnalysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
